# ðŸ¤– Transformer Usage in PlantDocBot

## Overview

This document provides a comprehensive analysis of how **Transformer models** are utilized in the PlantDocBot project for plant disease detection and classification.

---

## Table of Contents

- [Introduction](#introduction)
- [Transformer Architecture](#transformer-architecture)
- [Implementation Details](#implementation-details)
- [Model Training](#model-training)
- [API Integration](#api-integration)
- [Performance Metrics](#performance-metrics)
- [Use Cases](#use-cases)
- [Technical Stack](#technical-stack)
- [Code Examples](#code-examples)

---

## Introduction

PlantDocBot employs a **dual-model architecture** combining traditional Computer Vision (CNN) with modern Natural Language Processing (Transformers) to provide comprehensive plant disease detection capabilities.

### Why Transformers?

Transformers enable the system to:
- Understand natural language descriptions of plant symptoms
- Process complex symptom patterns and relationships
- Provide accurate disease classification from text input
- Complement image-based detection with text-based diagnosis

---

## Transformer Architecture

### Model Type: BERT (Bidirectional Encoder Representations from Transformers)

**BERT** is used for sequence classification tasks, specifically fine-tuned for plant disease detection from symptom descriptions.

### Key Components

1. **AutoTokenizer**
   - Converts text input into token IDs
   - Handles vocabulary mapping
   - Manages special tokens ([CLS], [SEP], [PAD])

2. **AutoModelForSequenceClassification**
   - Pre-trained BERT base model
   - Fine-tuned classification head for 38 plant disease classes
   - Outputs probability distribution over disease categories

3. **Label Encoder**
   - Maps predicted class IDs to disease names
   - Stored as `label_encoder.pkl`

---

## Implementation Details

### Library Version

```
transformers==4.36.0
```

### Dependencies

The transformer implementation requires:
- `transformers` - Hugging Face Transformers library
- `torch` - PyTorch backend
- `joblib` - For label encoder serialization
- `pandas` - Data preprocessing
- `scikit-learn` - Training utilities

### Model Location

```
Backend/models/text_classifier_model/
â”œâ”€â”€ config.json                 # Model configuration
â”œâ”€â”€ model.safetensors          # Model weights (safetensors format)
â”œâ”€â”€ tokenizer_config.json      # Tokenizer configuration
â”œâ”€â”€ vocab.txt                  # BERT vocabulary
â”œâ”€â”€ special_tokens_map.json    # Special token mappings
â””â”€â”€ label_encoder.pkl          # Class label encoder
```

---

## Model Training

### Training Pipeline (from `notebooks/TextClassifier.ipynb`)

#### 1. **Data Preparation**

```python
from transformers import AutoTokenizer

# Load pre-trained BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenize symptom descriptions
def tokenize_function(examples):
    return tokenizer(examples['text'], 
                     padding='max_length', 
                     truncation=True)
```

#### 2. **Model Initialization**

```python
from transformers import AutoModelForSequenceClassification

# Initialize BERT model with classification head
model = AutoModelForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=38  # 38 plant disease classes
)
```

#### 3. **Training Configuration**

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)
```

#### 4. **Data Collation**

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

#### 5. **Training Execution**

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

trainer.train()
```

### Training Dataset

- **Source**: PlantVillage dataset with text descriptions
- **Classes**: 38 plant disease categories
- **Features**: Symptom descriptions, disease characteristics
- **Preprocessing**: Tokenization, padding, truncation

---

## API Integration

### Backend Implementation (`Backend/main.py`)

#### Model Loading

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import joblib

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    str(text_model_path), 
    local_files_only=True
)

# Load model
text_model = AutoModelForSequenceClassification.from_pretrained(
    str(text_model_path), 
    local_files_only=True
).to(device)
text_model.eval()

# Load label encoder
encoder = joblib.load(str(label_encoder_path))
```

#### Prediction Function

```python
import torch.nn.functional as F

def predict_text(text: str):
    # Tokenize input
    inputs = tokenizer(
        text, 
        return_tensors="pt", 
        truncation=True, 
        padding=True
    ).to(device)
    
    # Get predictions
    with torch.no_grad():
        outputs = text_model(**inputs)
        probs = F.softmax(outputs.logits, dim=-1)
        confidence, pred_id = torch.max(probs, dim=-1)
    
    # Decode label
    label = encoder.inverse_transform([pred_id.item()])[0]
    confidence_score = round(confidence.item(), 4)
    
    # Generate recommendation
    recommendation = (
        "Your plant looks healthy ðŸŒ±"
        if "healthy" in label.lower()
        else "Your plant seems affected. Consider proper diagnosis and treatment."
    )
    
    return {
        "label": label, 
        "confidence": confidence_score, 
        "recommendation": recommendation
    }
```

#### API Endpoint

```python
from fastapi import FastAPI
from pydantic import BaseModel

class TextPredictionInputModel(BaseModel):
    input: str

@app.post("/text-prediction")
def text_predict_endpoint(input_data: TextPredictionInputModel):
    try:
        result = predict_text(input_data.input)
        return {"input_text": input_data.input, **result}
    except Exception as e:
        return {"error": str(e)}
```

---

## Performance Metrics

### Model Accuracy

- **Overall Accuracy**: ~92%
- **Training Dataset**: PlantVillage text descriptions
- **Validation Strategy**: Cross-validation
- **Inference Time**: < 100ms per prediction

### Comparison with CNN Model

| Model Type | Task | Accuracy | Use Case |
|------------|------|----------|----------|
| **CNN** | Image Classification | ~95% | Visual disease detection |
| **BERT Transformer** | Text Classification | ~92% | Symptom-based diagnosis |

---

## Use Cases

### 1. **Text-Based Symptom Analysis**

**Input Example:**
```
"My tomato plant has brown spots on leaves with yellow halos"
```

**Output:**
```json
{
  "label": "Tomato - Early Blight",
  "confidence": 0.8934,
  "recommendation": "Your plant seems affected. Consider proper diagnosis and treatment."
}
```

### 2. **Complementary Diagnosis**

Users can:
- Upload an image â†’ Get CNN prediction
- Describe symptoms â†’ Get BERT prediction
- Compare both results for higher confidence

### 3. **Accessibility**

- Users without cameras can describe symptoms
- Works in low-light conditions where images fail
- Supports detailed symptom descriptions

---

## Technical Stack

### Transformer-Related Dependencies

```txt
transformers==4.36.0          # Hugging Face Transformers
torch==2.9.0                  # PyTorch backend
torchvision==0.24.0           # Vision utilities
torchaudio==2.9.0             # Audio utilities
joblib==1.5.2                 # Model serialization
scikit-learn==1.7.2           # Label encoding
```

### Hardware Requirements

- **CPU**: Any modern processor (inference)
- **GPU**: CUDA-compatible GPU (recommended for training)
- **RAM**: 4GB minimum, 8GB recommended
- **Storage**: ~500MB for model files

---

## Code Examples

### Example 1: Basic Text Prediction

```python
# Input
symptom_description = "Yellow leaves with brown edges on potato plant"

# Prediction
result = predict_text(symptom_description)

# Output
print(f"Disease: {result['label']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Recommendation: {result['recommendation']}")
```

### Example 2: Batch Predictions

```python
symptoms = [
    "White powdery substance on grape leaves",
    "Black spots on apple fruit",
    "Healthy corn plant with green leaves"
]

for symptom in symptoms:
    result = predict_text(symptom)
    print(f"{symptom} â†’ {result['label']} ({result['confidence']:.2%})")
```

### Example 3: API Request

```bash
curl -X POST "http://localhost:8000/text-prediction" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "My tomato plant has curled yellow leaves"
  }'
```

**Response:**
```json
{
  "input_text": "My tomato plant has curled yellow leaves",
  "label": "Tomato - Yellow Leaf Curl Virus",
  "confidence": 0.8756,
  "recommendation": "Your plant seems affected. Consider proper diagnosis and treatment."
}
```

---

## Supported Disease Classes (38 Total)

The transformer model can classify the following plant diseases:

### Fruits
- **Apple**: Scab, Black Rot, Cedar Apple Rust, Healthy
- **Blueberry**: Healthy
- **Cherry**: Powdery Mildew, Healthy
- **Grape**: Black Rot, Esca, Leaf Blight, Healthy
- **Orange**: Huanglongbing (Citrus Greening)
- **Peach**: Bacterial Spot, Healthy
- **Strawberry**: Leaf Scorch, Healthy

### Vegetables
- **Corn**: Cercospora Leaf Spot, Common Rust, Northern Leaf Blight, Healthy
- **Pepper**: Bacterial Spot, Healthy
- **Potato**: Early Blight, Late Blight, Healthy
- **Squash**: Powdery Mildew
- **Tomato**: Bacterial Spot, Early Blight, Late Blight, Leaf Mold, Septoria Leaf Spot, Spider Mites, Target Spot, Mosaic Virus, Yellow Leaf Curl Virus, Healthy

### Others
- **Raspberry**: Healthy
- **Soybean**: Healthy

---

## Model Architecture Details

### BERT Base Configuration

```json
{
  "architectures": ["BertForSequenceClassification"],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 38,
  "type_vocab_size": 2,
  "vocab_size": 30522
}
```

### Model Parameters

- **Total Parameters**: ~110 million
- **Trainable Parameters**: ~110 million (full fine-tuning)
- **Embedding Dimension**: 768
- **Attention Heads**: 12
- **Transformer Layers**: 12

---

## Advantages of Using Transformers

### 1. **Contextual Understanding**
- BERT captures bidirectional context
- Understands relationships between symptoms
- Handles complex symptom descriptions

### 2. **Transfer Learning**
- Pre-trained on massive text corpus
- Fine-tuned on plant disease domain
- Requires less training data

### 3. **Flexibility**
- Accepts variable-length text input
- No fixed input format required
- Handles natural language variations

### 4. **State-of-the-Art Performance**
- 92% accuracy on plant disease classification
- Robust to spelling variations
- Generalizes well to unseen descriptions

---

## Future Enhancements

### Potential Improvements

1. **Model Upgrades**
   - Experiment with RoBERTa, ALBERT, or DistilBERT
   - Multi-lingual support for global accessibility
   - Domain-specific pre-training on agricultural texts

2. **Feature Additions**
   - Multi-label classification (multiple diseases)
   - Severity estimation from symptoms
   - Treatment recommendation generation

3. **Optimization**
   - Model quantization for faster inference
   - ONNX export for cross-platform deployment
   - Edge deployment for offline usage

4. **Data Augmentation**
   - Expand training dataset with more symptom variations
   - Include regional disease patterns
   - Incorporate expert-validated descriptions

---

## References

### Documentation
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [PlantVillage Dataset](https://plantvillage.psu.edu/)

### Related Files
- `Backend/main.py` - Production implementation
- `notebooks/TextClassifier.ipynb` - Training notebook
- `Backend/requirements.txt` - Dependencies
- `docs/API_DOCUMENTATION.md` - API reference

---

## Conclusion

The integration of **BERT transformers** in PlantDocBot represents a significant advancement in plant disease detection technology. By combining:

- **Computer Vision (CNN)** for image analysis
- **Natural Language Processing (BERT)** for symptom analysis
- **AI Chatbot (Gemini)** for interactive assistance

PlantDocBot provides a comprehensive, multi-modal solution for plant health management with **92% accuracy** in text-based disease classification.

---

**Last Updated**: December 24, 2025  
**Version**: 1.0  
**Author**: PlantDocBot Development Team

---

<div align="center">

**Made with ðŸ¤– and ðŸŒ¿**

For questions or contributions, visit the [GitHub Repository](https://github.com/karansrabcd01/AI_plantdocbot)

</div>
